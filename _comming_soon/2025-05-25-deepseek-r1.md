---
layout: post
title: DeepSeek-R1
category: LLM
date: 2025-05-25
---

### To Study 
- SFT
- Rejection Sampling
- DPO


### Abstract


DeepSeek-R1-Zero
- Model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step,
- Naturally emerges with numerous powerful and intriguing reasoning behaviors. 
- Encounters challenges such as poor readability, and language mixing. 
DeepSeek-R1
- Incorporates multi-stage training and cold-start data before RL. 

Relase six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.


